{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os, glob\n",
    "\n",
    "from preprocess import get_data\n",
    "from models.main import build_network\n",
    "from utils.utils import save_metrics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import os, glob\n",
    "\n",
    "def plot_event(args):\n",
    "    event = glob.glob(os.path.join(args.directory, 'event*'))\n",
    "    ea = event_accumulator.EventAccumulator(event[0])\n",
    "    ea.Reload() \n",
    "    ea.Tags()\n",
    "    metrics =  np.unique([metric.split('_')[0] for metric in ea.Tags()['scalars']])\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(metrics), figsize=(15,8))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axs[i].set_title(metric)\n",
    "        train = pd.DataFrame(ea.Scalars('{}_train'.format(metric))).value.values\n",
    "        val = pd.DataFrame(ea.Scalars('{}_val'.format(metric))).value.values\n",
    "        axs[i].plot(np.arange(len(train)), train, c='k', label='train')\n",
    "        axs[i].plot(np.arange(len(val)), val, c='b', label='val')\n",
    "        axs[i].set_title('Learning curve {}'.format(metric), fontsize=25)\n",
    "        axs[i].set_xlabel('Epoch', fontsize=20)\n",
    "        axs[i].set_ylabel(metric, fontsize=20)\n",
    "        axs[i].grid(True)\n",
    "        axs[i].legend(loc='best', fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    plt.savefig('{}/learning_curves.png'.format(args.directory))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_histogram(args, scores_in, scores_out):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.title('Inliers vs Outliers {}'.format(args.model), fontsize=16)\n",
    "    plt.hist(scores_in, label='Inliers', bins=25, density=True, histtype='step', color='b')\n",
    "    plt.hist(scores_out, label='Outliers', bins=25, density=True, histtype='step', color='r')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.savefig('{}/histogram.png'.format(args.directory))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_metrics(args, metric_name, score, m1, m2):\n",
    "    plt.title('{} curve. Score: {:.2f}'.format(metric_name, score), fontsize=20)\n",
    "    plt.plot(m1, m2)\n",
    "    if metric_name=='AU ROC':\n",
    "        m1_name = 'FPR'\n",
    "        m2_name = 'TPR'\n",
    "        ident=[0.0, 1.0]\n",
    "        plt.plot(ident, ident, c='r')\n",
    "    elif metric_name=='AU PR':\n",
    "        m1_name = 'Recall'\n",
    "        m2_name = 'Precision'\n",
    "        plt.plot([1.0, 0.0], [0.0, 1.0], c='r')\n",
    "    plt.xlabel(m1_name, fontsize=18)\n",
    "    plt.ylabel(m2_name, fontsize=18)\n",
    "    plt.grid()\n",
    "    plt.savefig('{}/{}.png'.format(args.directory, metric_name))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(args, scores, labels):\n",
    "    \"\"\"\n",
    "    Computing the Area under the curve ROC and PR.\n",
    "    \"\"\"\n",
    "    in_scores = scores[labels==0]\n",
    "    out_scores = scores[labels==1]\n",
    "\n",
    "    auroc, aupr = compute_roc_pr(args, in_scores, out_scores)\n",
    "    metrics = {'AU ROC': auroc,\n",
    "               'AU PR': aupr,\n",
    "               }\n",
    "    return metrics\n",
    "\n",
    "def compute_roc_pr(args, inliers_scores, outlier_scores):\n",
    "    auroc_score, fprs, tprs = auroc(inliers_scores, outlier_scores)\n",
    "    plot_metrics(args, 'AU ROC', auroc_score, fprs, tprs)\n",
    "    aupr_score, recalls, precisions = aupr(inliers_scores, outlier_scores)\n",
    "    plot_metrics(args, 'AU PR', aupr_score, recalls, precisions)\n",
    "    return auroc_score, aupr_score\n",
    "\n",
    "def auroc(in_scores, out_scores):\n",
    "    scores = np.concatenate((in_scores, out_scores), axis=0)\n",
    "    start = np.min(scores)\n",
    "    end = np.max(scores)   \n",
    "    gap = (end - start)/100000\n",
    "\n",
    "    tprs = []\n",
    "    fprs = []\n",
    "    for delta in np.arange(end, start, -gap):\n",
    "        tpr = np.sum(np.sum(out_scores >= delta)) / np.float(len(out_scores))\n",
    "        fpr = np.sum(np.sum(in_scores >= delta)) / np.float(len(in_scores))\n",
    "        tprs.append(tpr)\n",
    "        fprs.append(fpr)\n",
    "    return auc(fprs, tprs), fprs, tprs\n",
    "\n",
    "def aupr(in_scores, out_scores):\n",
    "    scores = np.concatenate((in_scores, out_scores), axis=0)\n",
    "    start = np.min(scores)\n",
    "    end = np.max(scores)   \n",
    "    gap = (end - start)/100000\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for delta in np.arange(end, start, -gap):\n",
    "        tp = np.sum(np.sum(out_scores >= delta)) #/ np.float(len(out_scores))\n",
    "        fp = np.sum(np.sum(in_scores >= delta)) #/ np.float(len(in_scores))\n",
    "        if tp + fp == 0: continue\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / np.float(len(out_scores))\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    return auc(recalls, precisions), recalls, precisions\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for metric, value in metrics.items():\n",
    "        print(\"{}: {:.3f}\".format(metric, value))\n",
    "    print(\"##########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, dataloader):\n",
    "    \"\"\"Evaluting the anomaly detection model.\"\"\"\n",
    "    model = build_network(args).to(args.device)\n",
    "    ### Loading the trained model...\n",
    "    state_dict = torch.load('{}/trained_parameters.pth'.format(args.directory))\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    scores = []\n",
    "    out_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, x, _, y_out in dataloader:\n",
    "            x = x.float().to(args.device)\n",
    "\n",
    "            score = model.compute_anomaly_score(x)\n",
    "            scores.append(score.detach().cpu())\n",
    "            out_labels.append(y_out.cpu())\n",
    "    \n",
    "    scores = torch.cat(scores).numpy()\n",
    "    out_labels = torch.cat(out_labels).numpy()\n",
    "    \n",
    "    metrics = compute_metrics(args, scores, out_labels)\n",
    "    plot_histogram(args, scores[out_labels==0], scores[out_labels==1])\n",
    "    print_metrics(metrics)\n",
    "    save_metrics(metrics, args.directory, 'test')\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    r='./experiments'\n",
    "    z_dim=128\n",
    "    in_dim=152\n",
    "    data_pth='../data'\n",
    "    batch_size=128\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model = 'ae'\n",
    "args.hierClass = 'Periodic'\n",
    "args.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEP\n",
      "AU ROC 0.564 +- 0.024\n",
      "AU PR 0.133 +- 0.015\n",
      "DSCT\n",
      "AU ROC 0.367 +- 0.015\n",
      "AU PR 0.073 +- 0.002\n",
      "E\n",
      "AU ROC 0.864 +- 0.009\n",
      "AU PR 0.650 +- 0.015\n",
      "RRL\n",
      "AU ROC 0.907 +- 0.015\n",
      "AU PR 0.815 +- 0.035\n",
      "LPV\n",
      "AU ROC 0.996 +- 0.000\n",
      "AU PR 0.987 +- 0.001\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if args.hierClass=='Transient':\n",
    "    possible_outliers = ['SLSN',\n",
    "                         'SNII',\n",
    "                         'SNIa',\n",
    "                         'SNIbc']\n",
    "elif args.hierClass == 'Stochastic':\n",
    "    possible_outliers = [ 'AGN' ,\n",
    "                         'Blazar',\n",
    "                         'CV/Nova',\n",
    "                         'QSO',\n",
    "                         'YSO']\n",
    "elif args.hierClass == 'Periodic':\n",
    "    possible_outliers = ['CEP',\n",
    "                         'DSCT',\n",
    "                         'E',\n",
    "                         'RRL', \n",
    "                         'LPV']\n",
    "\n",
    "auprs = []\n",
    "auprs_devs = []\n",
    "aurocs = []\n",
    "aurocs_devs = []\n",
    "\n",
    "for outlier in possible_outliers:\n",
    "    auroc = []\n",
    "    auroc_dev = []\n",
    "    aupr = []\n",
    "    aupr_dev = []\n",
    "    for fold in range(5):\n",
    "        args.fold = fold\n",
    "        args.outlier = outlier\n",
    "        \n",
    "        job_name = '{}_{}_{}_lr{}_ld{}_fold{}'.format(\n",
    "                   args.model, args.hierClass, args.outlier, args.lr, args.z_dim, args.fold)\n",
    "\n",
    "        args.directory = os.path.join(args.r, job_name)\n",
    "\n",
    "        #_, _, dataloader_test = get_data(args)\n",
    "        #test(args, dataloader_test)\n",
    "        data = json.load(open('{}/metrics_test.json'.format(args.directory)))\n",
    "        auroc.append(data['AU ROC'])\n",
    "        aupr.append(data['AU PR'])\n",
    "    aurocs.append(np.mean(auroc))\n",
    "    aurocs_devs.append(np.std(auroc))\n",
    "    auprs.append(np.mean(aupr))\n",
    "    auprs_devs.append(np.std(aupr))\n",
    "    print(outlier)\n",
    "    print('AU ROC {:.3f} +- {:.3f}'.format(aurocs[-1], aurocs_devs[-1]))\n",
    "    print('AU PR {:.3f} +- {:.3f}'.format(auprs[-1], auprs_devs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
